{
 "cells": [
  {
   "cell_type": "raw",
   "id": "80ffa616-7820-4dd3-8afc-e8a264b12661",
   "metadata": {},
   "source": [
    "The framework for the pipeline is from Xander Wilcke and can be found here: \n",
    "https://gitlab.com/wxwilcke/graphsynth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198b1fb4-dbef-4e5b-9e48-d63a4d5a122c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment if running in colab\n",
    "# !pip install rdflib\n",
    "# !pip install torch_geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6c1eac-1520-4564-9173-4749e2f002ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdflib import Graph, URIRef, BNode, Literal\n",
    "from rdflib.namespace import FOAF, RDF\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw\n",
    "import random \n",
    "from collections import Counter\n",
    "from itertools import combinations\n",
    "import matplotlib.pyplot as plt\n",
    "import statistics\n",
    "import sys\n",
    "import wave\n",
    "import math\n",
    "import struct\n",
    "import argparse\n",
    "from itertools import *\n",
    "import noise1\n",
    "import target1\n",
    "import exp\n",
    "import regex as re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53a1f28-a801-4011-bfdb-6f8dc09d7f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from base64 import urlsafe_b64encode\n",
    "import gzip\n",
    "from io import BytesIO\n",
    "from rdf import Triple, URIRef, Literal\n",
    "import datetime\n",
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06743140-5cc2-45e2-9b59-5fee92ceb3d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from collections import defaultdict, Counter\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "import pandas as pd\n",
    "from torch_geometric.utils import to_networkx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3f8bb9-9af5-48a5-815b-c15ea17abdde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocabulary retrieved from http://svnweb.freebsd.org/csrg/share/dict/words?view=co&content-type=text/plain\n",
    "_STRING_VOCAB = \"./vocab.txt.gz\"\n",
    "_ATTR_AVAILABLE = [\"xsd.string\", \"xsd.gYear\", \"xsd.integer\", \"xsd.float\", \n",
    "                    \"ogc.wktLiteral\", \"blob.image\", \"xsd.date\",\n",
    "                    \"xsd.dateTime\", \"xsd.anyURI\", \"blob.audio\"]\n",
    "_ATTR_INCLUDED = [attr for attr in _ATTR_AVAILABLE]\n",
    "_NAMESPACE = \"https://example.org/\"\n",
    "_NAMESPACE_XSD = \"http://www.w3.org/2001/XMLSchema#\"\n",
    "_NAMESPACE_OGC = \"http://www.opengis.net/ont/geosparql#\"\n",
    "_NAMESPACE_KGB = \"http://kgbench.info/dt#\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0918453-3888-4196-9fb0-35c56498b37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def im_to_b64(im):\n",
    "    buff = BytesIO()\n",
    "    im.save(buff, format=\"JPEG\")\n",
    "    b64img = urlsafe_b64encode(buff.getvalue())\n",
    "\n",
    "    return b64img.decode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff737304-ec4b-48ce-9d51-c610b7cd9108",
   "metadata": {},
   "outputs": [],
   "source": [
    "def audio_to_b64_rand(nchannels, sampwidth, framerate, nframes):\n",
    "    audio = noise1.gen_audio(44100,0.1,2, nsamples=None)\n",
    "    w = wave.open('rand.wav','w')\n",
    "    w.setparams((nchannels,sampwidth,framerate,nframes,'NONE', 'not compressed'))\n",
    "    w.writeframesraw(audio)\n",
    "    w.close()\n",
    "    b64audio = urlsafe_b64encode(audio)\n",
    "    \n",
    "    return b64audio.decode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b10640-35a0-494b-b5ea-9df3fd5b1e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def audio_to_b64_targ(assignment, nchannels, sampwidth, framerate, nframes):\n",
    "    audio = target1.gen_audio(assignment,44100,0.1)\n",
    "    w = wave.open('targ.wav', 'w')\n",
    "    #w.setparams((nchannels, sampwidth, framerate, nframes, 'NONE', 'not compressed'))\n",
    "    w.setparams((2,2,44100,44100,'NONE', 'not compressed'))\n",
    "    w.writeframesraw(audio)\n",
    "    w.close()\n",
    "    b64audio = urlsafe_b64encode(audio)\n",
    "\n",
    "    return b64audio.decode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e126251-2e29-4ad3-96e8-ccec1ba18eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_vocab():\n",
    "    vocab = list()\n",
    "    with gzip.open(_STRING_VOCAB, 'rt') as f:\n",
    "        vocab = [word for word in f.read().split('\\n') if len(word) >= 5]\n",
    "\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9b94ca-a3f7-4b82-846d-9e9ca586e607",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = gen_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6adacf-615b-44c9-9c20-6bc20fec6d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_random_attr(attr, vocab):\n",
    "    if attr == \"xsd.string\":\n",
    "        o = Literal(noise1.gen_string(vocab), dtype=_NAMESPACE_XSD+\"string\")\n",
    "    elif attr == \"xsd.anyURI\":\n",
    "        o = Literal(noise1.gen_anyURI(vocab), dtype=_NAMESPACE_XSD+\"anyURI\")\n",
    "    elif attr == \"xsd.gYear\":\n",
    "        o = Literal(str(noise1.gen_gYear()), dtype=_NAMESPACE_XSD+\"gYear\")\n",
    "    elif attr == \"xsd.date\":\n",
    "        o = Literal(str(noise1.gen_date()), dtype=_NAMESPACE_XSD+\"date\")\n",
    "    elif attr == \"xsd.dateTime\":\n",
    "        o = Literal(str(noise1.gen_dateTime()), dtype=_NAMESPACE_XSD+\"dateTime\")\n",
    "    elif attr == \"xsd.integer\":\n",
    "        o = Literal(str(noise1.gen_integer()), dtype=_NAMESPACE_XSD+\"integer\")\n",
    "    elif attr == \"xsd.float\":\n",
    "        o = Literal(str(noise1.gen_float()), dtype=_NAMESPACE_XSD+\"float\")\n",
    "    elif attr == \"xsd.boolean\":\n",
    "        o = Literal(str(noise1.gen_boolean()), dtype=_NAMESPACE_XSD+\"boolean\")\n",
    "    elif attr == \"blob.image\":\n",
    "        o = Literal(im_to_b64(noise1.gen_image()), dtype=_NAMESPACE_KGB+\"base64Image\")\n",
    "    elif attr == \"ogc.wktLiteral\":\n",
    "        o = Literal(noise1.gen_wktLiteral(), dtype=_NAMESPACE_OGC+\"wktLiteral\")\n",
    "    elif attr == \"blob.audio\":\n",
    "        o = Literal(audio_to_b64_rand(2,2,44100,44100), dtype=_NAMESPACE_XSD+\"base64Audio\")\n",
    "    else:\n",
    "        o = Literal(\"\")  # empty literal\n",
    "\n",
    "    return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a9ce52-1caf-4938-b887-23f965c5e280",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_target_attr(attr, vocab, assignment, nclusters):\n",
    "    if attr == \"xsd.string\":\n",
    "        o = Literal(target1.gen_string(vocab, assignment, nclusters), dtype=_NAMESPACE_XSD+\"string\")\n",
    "    elif attr == \"xsd.anyURI\":\n",
    "        o = Literal(target1.gen_anyURI(vocab, assignment, nclusters), dtype=_NAMESPACE_XSD+\"anyURI\")\n",
    "    elif attr == \"xsd.gYear\":\n",
    "        o = Literal(str(target1.gen_gYear(assignment, nclusters)), dtype=_NAMESPACE_XSD+\"gYear\")\n",
    "    elif attr == \"xsd.date\":\n",
    "        o = Literal(str(target1.gen_date(assignment, nclusters)), dtype=_NAMESPACE_XSD+\"date\")\n",
    "    elif attr == \"xsd.dateTime\":\n",
    "        o = Literal(str(target1.gen_dateTime(assignment, nclusters)), dtype=_NAMESPACE_XSD+\"dateTime\")\n",
    "    elif attr == \"xsd.integer\":\n",
    "        o = Literal(str(target1.gen_integer(assignment, nclusters)), dtype=_NAMESPACE_XSD+\"integer\")\n",
    "    elif attr == \"xsd.float\":\n",
    "        o = Literal(str(target1.gen_float(assignment, nclusters)), dtype=_NAMESPACE_XSD+\"float\")\n",
    "    elif attr == \"xsd.boolean\":\n",
    "        o = Literal(str(target1.gen_boolean(assignment)), dtype=_NAMESPACE_XSD+\"boolean\")\n",
    "    elif attr == \"blob.image\":\n",
    "        o = Literal(im_to_b64(target1.gen_image(assignment, nclusters)), dtype=_NAMESPACE_KGB+\"base64Image\")\n",
    "    elif attr == \"ogc.wktLiteral\":\n",
    "        o = Literal(target1.gen_wktLiteral(assignment, nclusters), dtype=_NAMESPACE_OGC+\"wktLiteral\")\n",
    "    elif attr == \"blob.audio\":\n",
    "        o = Literal(audio_to_b64_targ(assignment,2,2,44100,44100), dtype=_NAMESPACE_XSD+\"base64Audio\")\n",
    "    else:\n",
    "        o = Literal(\"\")  # empty literal\n",
    "\n",
    "\n",
    "    return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64ba2d9-9637-4d92-9399-73bf4b323c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generate KG with desired parameter values, class distributions, attribute values, etc.\n",
    "\n",
    "def generate(n_nodes, num_relations, num_targets,\n",
    "             num_target_clusters,\n",
    "             target_cluster_distribution, cluster_strength = 1.0,\n",
    "             p_attr=0.9,density=None, symm=None, reflex=None,\n",
    "             in_deg=None, in_ratio=None,\n",
    "             out_deg=None, out_ratio=None,\n",
    "             n_hops=None, hops_ratio=None,\n",
    "             clustering=None, \n",
    "             diameter=None,\n",
    "             avg_short_path=None, avg_long_path=None):\n",
    "    assert len(target_cluster_distribution) == num_target_clusters\n",
    "\n",
    "    g, edges = noise1.gen_structure(n_nodes, density=density, symm=symm, reflex=reflex,\n",
    "             in_deg=in_deg, in_ratio=in_ratio,\n",
    "             out_deg=out_deg, out_ratio=out_ratio,\n",
    "             n_hops=n_hops, hops_ratio=hops_ratio,\n",
    "             clustering=clustering, \n",
    "             diameter=diameter,\n",
    "             avg_short_path=avg_short_path, avg_long_path=avg_long_path)\n",
    "    \n",
    "    context = set()\n",
    "    if num_relations<3:\n",
    "        for s, o in edges:\n",
    "            p = np.random.choice(range(num_relations))\n",
    "            t = Triple(URIRef(_NAMESPACE + \"NODE_\" + str(s)),\n",
    "                       URIRef(_NAMESPACE + \"edge_\" + str(p)),\n",
    "                       URIRef(_NAMESPACE + \"NODE_\" + str(o)))\n",
    "\n",
    "            context.add(t.ntriple().encode())\n",
    "        \n",
    "    else:\n",
    "        for s, o in edges:\n",
    "            if (o,s) in edges and s!=o:\n",
    "                t = Triple(URIRef(_NAMESPACE + \"NODE_\" + str(s)),\n",
    "                       URIRef(_NAMESPACE + \"edge_\" + str(num_relations-1)),\n",
    "                       URIRef(_NAMESPACE + \"NODE_\" + str(o)))\n",
    "            elif s==o:\n",
    "                t = Triple(URIRef(_NAMESPACE + \"NODE_\" + str(s)),\n",
    "                       URIRef(_NAMESPACE + \"edge_\" + str(num_relations-2)),\n",
    "                       URIRef(_NAMESPACE + \"NODE_\" + str(o)))\n",
    "            else:\n",
    "                p = np.random.choice(range(num_relations-2))\n",
    "                t = Triple(URIRef(_NAMESPACE + \"NODE_\" + str(s)),\n",
    "                           URIRef(_NAMESPACE + \"edge_\" + str(p)),\n",
    "                           URIRef(_NAMESPACE + \"NODE_\" + str(o)))\n",
    "            context.add(t.ntriple().encode())\n",
    "    assert n_nodes >= num_targets\n",
    "    instances = np.arange(n_nodes, dtype=np.int32)\n",
    "    np.random.shuffle(instances)\n",
    "\n",
    "    target_entities_idx = instances[:num_targets]\n",
    "    non_target_entities_idx = instances[num_targets:]    \n",
    "    attr_relations = dict()\n",
    "    for attr in _ATTR_INCLUDED:\n",
    "        attr_relations[attr] = np.random.choice(range(1,4))\n",
    "    \n",
    "    # add attributes to non-target entities\n",
    "    for s in non_target_entities_idx:\n",
    "        for attr in _ATTR_INCLUDED:\n",
    "            if np.random.rand() > p_attr:\n",
    "                continue\n",
    "            attr_rel = np.random.choice(range(1, attr_relations[attr]+1))\n",
    "            t = Triple(URIRef(_NAMESPACE + \"NODE_\" + str(s)),\n",
    "                       URIRef(_NAMESPACE + \"edge_\" + str(attr) + '_' + str(attr_rel)),\n",
    "                       gen_random_attr(attr, vocab))\n",
    "\n",
    "            context.add(t.ntriple().encode())\n",
    "            \n",
    "    # assign node labels starting from seeding process and add attributes to targets\n",
    "    targets = [list() for _ in range(num_target_clusters)]\n",
    "    counter = 0\n",
    "    class_assign = dict()\n",
    "    class_counts = dict()\n",
    "    class_targ = dict()\n",
    "    class_ratio = dict()\n",
    "    n_num_target_clusters = num_target_clusters\n",
    "    c = 0\n",
    "    for i in target_cluster_distribution:\n",
    "        class_targ[str(c)] = int(i * n_nodes)+1\n",
    "        class_ratio[str(c)] = i\n",
    "        class_counts[str(c)] = 0\n",
    "        c+=1\n",
    "    k=0\n",
    "    for s in target_entities_idx:\n",
    "        if counter<= num_target_clusters-1:\n",
    "            assignment = k%(num_target_clusters)            \n",
    "            class_assign[str(s)] = assignment\n",
    "            class_counts[str(assignment)] = class_counts.get(str(assignment))+1\n",
    "            nearby = [n for n in nx.generators.ego_graph(g, s, radius=4)]\n",
    "            inc = [n[0] for n in g.in_edges(s)]\n",
    "            nearby = nearby + inc\n",
    "            nearby = [n for n in nearby if class_assign.get(str(n)) == None]\n",
    "            if len(nearby) > 0:\n",
    "                cluster_max = int((n_nodes * target_cluster_distribution[k])/10)\n",
    "                if len(nearby) > cluster_max:\n",
    "                    nearby = np.random.choice(nearby, size=cluster_max, replace=False)\n",
    "                    for j in nearby:\n",
    "                        if class_assign.get(str(j)) ==None:\n",
    "                            if np.random.rand() < cluster_strength:\n",
    "                                class_assign[str(j)] = assignment\n",
    "                                class_counts[str(assignment)] = class_counts.get(str(assignment))+1\n",
    "                else:\n",
    "                    for j in nearby:\n",
    "                        if class_assign.get(str(j)) ==None:\n",
    "                            if np.random.rand() < cluster_strength:\n",
    "                                class_assign[str(j)] = assignment\n",
    "                                class_counts[str(assignment)] = class_counts.get(str(assignment))+1\n",
    "            k += 1\n",
    "            counter+=1\n",
    "        else:\n",
    "            if class_assign.get(str(s))!= None:\n",
    "                assignment = class_assign[str(s)]\n",
    "            else:\n",
    "                exclude = []\n",
    "                for i in range(len(class_targ)):\n",
    "                    if class_targ.get(str(i)) < class_counts.get(str(i)) and len(n_target_cluster_distribution)>1:\n",
    "                        exclude.append(i)\n",
    "                for i in exclude:\n",
    "                    class_ratio.pop(str(i), None)\n",
    "                n_num_target_clusters = [int(n) for n in class_ratio.keys()]\n",
    "                n_target_cluster_distribution = [n for n in class_ratio.values()]\n",
    "                n_target_cluster_distribution = np.asarray(n_target_cluster_distribution).astype('float64')\n",
    "                n_target_cluster_distribution = n_target_cluster_distribution / np.sum(n_target_cluster_distribution)\n",
    "                neighbors = [n for n in g.neighbors(s)]\n",
    "                if len(neighbors) > 0:\n",
    "                    dig = []\n",
    "                    for i in neighbors:\n",
    "                        b = class_assign.get(str(i))\n",
    "                        if b != None and (b not in exclude):\n",
    "                            dig.append(b)\n",
    "                    if len(dig) > 0:\n",
    "                        r = statistics.mode(dig)\n",
    "                        if np.random.rand() < cluster_strength:\n",
    "                            assignment = np.int32(r)\n",
    "                            class_assign[str(s)] = assignment\n",
    "                            class_counts[str(assignment)] = class_counts.get(str(assignment))+1\n",
    "                        else:\n",
    "                            assignment = np.random.choice(n_num_target_clusters,\n",
    "                                          p=n_target_cluster_distribution)\n",
    "                            class_assign[str(s)] = assignment\n",
    "                            class_counts[str(assignment)] = class_counts.get(str(assignment))+1\n",
    "                    else:\n",
    "                        assignment = np.random.choice(n_num_target_clusters,\n",
    "                                      p=n_target_cluster_distribution)\n",
    "                        class_assign[str(s)] = assignment\n",
    "                        class_counts[str(assignment)] = class_counts.get(str(assignment))+1\n",
    "                else:\n",
    "                    assignment = np.random.choice(n_num_target_clusters,\n",
    "                                  p=n_target_cluster_distribution)\n",
    "                    class_assign[str(s)] = assignment\n",
    "                    class_counts[str(assignment)] = class_counts.get(str(assignment))+1\n",
    "                    \n",
    "        for attr in _ATTR_INCLUDED:\n",
    "            if np.random.rand() > p_attr:\n",
    "                continue\n",
    "\n",
    "            attr_rel = np.random.choice(range(1, attr_relations[attr]+1))\n",
    "\n",
    "            t = Triple(URIRef(_NAMESPACE + \"NODE_\" + str(s)),\n",
    "                       URIRef(_NAMESPACE + \"edge_\" + str(attr) + '_' + str(attr_rel)),\n",
    "                       gen_target_attr(attr, vocab,\n",
    "                                       assignment, num_target_clusters))\n",
    "\n",
    "            context.add(t.ntriple().encode())\n",
    "\n",
    "        # add target signal\n",
    "        t = Triple(URIRef(_NAMESPACE + \"NODE_\" + str(s)),\n",
    "                   URIRef(_NAMESPACE + \"hasClass\"),\n",
    "                   URIRef(_NAMESPACE + \"NODE_CLASS_\" + str(assignment)))\n",
    "\n",
    "        targets[assignment].append(t.ntriple().encode())\n",
    "\n",
    "    with gzip.open(\"./context.nt.gz\", \"wb\") as gf:\n",
    "        gf.write(b'\\n'.join(context) + b'\\n')   \n",
    "    return(context, targets), g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e425d80b-2d63-4a10-b6a8-c3b63705bb70",
   "metadata": {},
   "outputs": [],
   "source": [
    "## run for normal synthetic KGs\n",
    "\n",
    "# num_nodes = 100\n",
    "# num_rel = 4\n",
    "# num_targs = 100\n",
    "# kg, G1 = generate(num_nodes,num_rel,num_targs,num_target_clusters=7, cluster_strength=1.0,\n",
    "#               target_cluster_distribution=[.143,.143,.143,.143,.143,.143,.142],\n",
    "#               density=0.00144, symm=None, reflex=None,\n",
    "#              in_deg=None, in_ratio=None,\n",
    "#              out_deg=None, out_ratio=None,\n",
    "#              n_hops=None, hops_ratio=None,\n",
    "#              clustering=None, \n",
    "#              diameter=None,\n",
    "#              avg_short_path=None, avg_long_path=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71a7cdb-99c8-4efa-b053-ba0defefbada",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## run for Cora-syn\n",
    "\n",
    "num_nodes = 2708\n",
    "num_rel = 1\n",
    "num_targs = 2708\n",
    "kg, G1 = generate(num_nodes,num_rel,num_targs,num_target_clusters=7, cluster_strength=1.0,\n",
    "              target_cluster_distribution=[.13,.08,.15,.30,.16,.11,.07],\n",
    "              density=0.00144, symm=None, reflex=None,\n",
    "             in_deg=None, in_ratio=0.01,\n",
    "             out_deg=None, out_ratio=0.01,\n",
    "             n_hops=None, hops_ratio=None,\n",
    "             clustering= 0.240673298501937, \n",
    "             diameter=None,\n",
    "             avg_short_path=None, avg_long_path=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f430c6-81ee-4960-984c-40125d5c21d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_network(data):\n",
    "    # visualize graph structure\n",
    "    \n",
    "    G = to_networkx(data, to_undirected=False)\n",
    "    pos = nx.spring_layout(G, seed=42)\n",
    "    cent = nx.degree_centrality(G)\n",
    "    node_size = list(map(lambda x: x * 500, cent.values()))\n",
    "    cent_array = np.array(list(cent.values()))\n",
    "    threshold = sorted(cent_array, reverse=True)[10]\n",
    "    cent_bin = np.where(cent_array >= threshold, 1, 0.1)\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    nodes = nx.draw_networkx_nodes(G, pos, node_size=node_size,\n",
    "                                   cmap=plt.cm.plasma,\n",
    "                                   node_color=cent_bin,\n",
    "                                   nodelist=list(cent.keys()),\n",
    "                                   alpha=cent_bin)\n",
    "    edges = nx.draw_networkx_edges(G, pos, width=0.25, alpha=0.3, arrowsize = 1)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd95a98-ede4-44e9-b3a0-a4a82a6640d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_classes(data):\n",
    "    # visualize class clustering\n",
    "                 \n",
    "    G = to_networkx(data, to_undirected=False)\n",
    "\n",
    "    label_dict = {\n",
    "        0: 0,\n",
    "        1: 1,\n",
    "        2: 2,\n",
    "        3: 3,\n",
    "        4: 4,\n",
    "        5: 5,\n",
    "        6: 6}\n",
    "    node_color = []\n",
    "    nodelist = [[], [], [], [], [], [], []]\n",
    "    colorlist = ['#e41a1c', '#377eb8', '#4daf4a', '#984ea3', '#ff7f00', '#ffff33', '#a65628']\n",
    "    labels = data.y\n",
    "    for n, i in enumerate(labels):\n",
    "        node_color.append(colorlist[i])\n",
    "        nodelist[i].append(n)\n",
    "    pos = nx.spring_layout(G, seed = 42)\n",
    "    plt.figure(figsize = (10, 10))\n",
    "    labellist = list(label_dict.values())\n",
    "    for num, i in enumerate(zip(nodelist, labellist)):\n",
    "        n, l = i[0], i[1]\n",
    "        nx.draw_networkx_nodes(G, pos, nodelist=n, node_size = 5, node_color = colorlist[num], label=l)\n",
    "    nx.draw_networkx_edges(G, pos, width = 0.25, arrowsize = 1)\n",
    "    plt.legend(bbox_to_anchor=(1, 1), loc='upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc4edd2-4177-44ff-a849-139d71409a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(data.num_node_features, 16)\n",
    "        self.conv2 = GCNConv(16, data.num_classes)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a4ec5c-34d6-44f7-b843-93dd84a17a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for i in range(10):\n",
    "    train,test,valid = exp.split(kg)\n",
    "    with gzip.open(\"./train.nt.gz\", \"wb\") as gf:\n",
    "        gf.write(b'\\n'.join(train) + b'\\n')\n",
    "    with gzip.open(\"./test.nt.gz\", \"wb\") as gf:\n",
    "        gf.write(b'\\n'.join(test) + b'\\n')\n",
    "    with gzip.open(\"./valid.nt.gz\", \"wb\") as gf:\n",
    "        gf.write(b'\\n'.join(valid) + b'\\n')\n",
    "        \n",
    "    train, test, valid = exp.node_ind(train,test,valid)\n",
    "    labs,nodes = exp.labels(kg)\n",
    "    tr_mask, tes_mask, val_mask = exp.masks(nodes,train,test,valid)\n",
    "    edge_index, edge_feats = exp.edges_info(kg,nodes)\n",
    "    x = torch.tensor(np.identity(len(nodes)),dtype=torch.float)\n",
    "    data = Data(x = x, edge_index=edge_index, \n",
    "              y =labs, train_mask= tr_mask, test_mask=tes_mask, val_mask = val_mask, \n",
    "                num_classes=7, num_nodes=num_nodes)\n",
    "    \n",
    "    # show visuals for graph\n",
    "    if i == 0:\n",
    "        exp.class_dist(labs, tr_mask, tes_mask, val_mask)\n",
    "        plot_network(data)\n",
    "        plot_classes(data)\n",
    "        counter = Counter(data.y.numpy())\n",
    "        counter = dict(counter)\n",
    "        count = [x[1] for x in sorted(counter.items())]\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.bar(range(7), count)\n",
    "        plt.title(\"Class distribution of node labels\")\n",
    "        plt.xlabel(\"Class\")\n",
    "        plt.ylabel(\"Total nodes\")\n",
    "        plt.show()\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = GCN()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "    model.train()\n",
    "    val_accs = []\n",
    "    for epoch in range(200):\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        model.eval()\n",
    "        pred = model(data).argmax(dim=1)\n",
    "        correct = (pred[data.val_mask] == data.y[data.val_mask]).sum()\n",
    "        accuracy = int(correct) / int(data.test_mask.sum())\n",
    "        val_accs.append(accuracy)\n",
    "    model.eval()\n",
    "    pred = model(data).argmax(dim=1)\n",
    "    correct = (pred[data.test_mask] == data.y[data.test_mask]).sum()\n",
    "    acc = int(correct) / int(data.test_mask.sum())\n",
    "    results.append(acc)\n",
    "    print(f'Accuracy: {acc:.4f}')\n",
    "    plt.figure(figsize=(12,8))\n",
    "    plt.plot(np.arange(1, len(val_accs) + 1), val_accs, label='Validation accuracy', c='blue')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accurarcy')\n",
    "    plt.title('GCN validation performance')\n",
    "    plt.legend(loc='upper right', fontsize='x-large')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920a6a46-da61-4fed-8930-1972aa8ffa2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cora dataset\n",
    "\n",
    "from torch_geometric.datasets import Planetoid\n",
    "\n",
    "dataset = Planetoid(root='/tmp/Cora', name='Cora')\n",
    "data = dataset[0]\n",
    "x = torch.tensor(np.identity(2708),dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afb52b1-58fd-4f0c-8efa-93cc2a4981c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ba2cd7-b0a7-4409-8920-eb8cdcb13ea9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = Data(x = x, edge_index=dataset[0].edge_index, y =dataset[0].y, train_mask= dataset[0].train_mask, test_mask=dataset[0].test_mask, val_mask = dataset[0].val_mask, num_classes=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf3bd6d-9324-4883-81d3-7984964c683d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(data.num_node_features, 16)\n",
    "        self.conv2 = GCNConv(16, 7)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c76557d-968b-4f01-a374-425a35ddfa86",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = GCN().to(device)\n",
    "data = data.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "rs = []\n",
    "for i in range(10):\n",
    "    model.train()\n",
    "    for epoch in range(100):\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    model.eval()\n",
    "    pred = model(data).argmax(dim=1)\n",
    "    correct = (pred[data.test_mask] == data.y[data.test_mask]).sum()\n",
    "    acc = int(correct) / int(data.test_mask.sum())\n",
    "    rs.append(acc)\n",
    "    print(f'Accuracy: {acc:.4f}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0af5b24-1ec7-409c-845b-044a9d07a6b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Plot Cora network\n",
    "## code from: https://medium.com/mlearning-ai/ultimate-guide-to-graph-neural-networks-1-cora-dataset-37338c04fe6f\n",
    "\n",
    "G = to_networkx(data, to_undirected=True)\n",
    "pos = nx.spring_layout(G, seed=42)\n",
    "cent = nx.degree_centrality(G)\n",
    "node_size = list(map(lambda x: x * 500, cent.values()))\n",
    "cent_array = np.array(list(cent.values()))\n",
    "threshold = sorted(cent_array, reverse=True)[10]\n",
    "print(\"threshold\", threshold)\n",
    "cent_bin = np.where(cent_array >= threshold, 1, 0.1)\n",
    "plt.figure(figsize=(12, 12))\n",
    "nodes = nx.draw_networkx_nodes(G, pos, node_size=node_size,\n",
    "                               cmap=plt.cm.plasma,\n",
    "                               node_color=cent_bin,\n",
    "                               nodelist=list(cent.keys()),\n",
    "                               alpha=cent_bin)\n",
    "edges = nx.draw_networkx_edges(G, pos, width=0.25, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bec9ac7-a4e1-45a3-9738-a67e1c43ad77",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Plot Cora classes\n",
    "## code from: https://medium.com/mlearning-ai/ultimate-guide-to-graph-neural-networks-1-cora-dataset-37338c04fe6f\n",
    "\n",
    "label_dict = {\n",
    "        0: 0,\n",
    "        1: 1,\n",
    "        2: 2,\n",
    "        3: 3,\n",
    "        4: 4,\n",
    "        5: 5,\n",
    "        6: 6}\n",
    "G = to_networkx(data, to_undirected=True)\n",
    "node_color = []\n",
    "nodelist = [[], [], [], [], [], [], []]\n",
    "colorlist = ['#e41a1c', '#377eb8', '#4daf4a', '#984ea3', '#ff7f00', '#ffff33', '#a65628']\n",
    "labels = data.y\n",
    "for n, i in enumerate(labels):\n",
    "    node_color.append(colorlist[i])\n",
    "    nodelist[i].append(n)\n",
    "pos = nx.spring_layout(G, seed = 42)\n",
    "plt.figure(figsize = (10, 10))\n",
    "labellist = list(label_dict.values())\n",
    "for num, i in enumerate(zip(nodelist, labellist)):\n",
    "    n, l = i[0], i[1]\n",
    "    nx.draw_networkx_nodes(G, pos, nodelist=n, node_size = 5, node_color = colorlist[num], label=l)\n",
    "nx.draw_networkx_edges(G, pos, width = 0.25)\n",
    "plt.legend(bbox_to_anchor=(1, 1), loc='upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa319a48-8c9b-476c-a9a3-f310ea1266e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Visualize results from experiments\n",
    "\n",
    "large = np.array([0.647,0.6514,0.6464,0.6534,0.6572,0.652,0.6674,0.6558,0.6604,0.6634])\n",
    "medium = np.array([0.5803,0.5808,0.5963,0.5753,0.5669,0.5878,0.5778,0.5898,0.5753,0.5763])\n",
    "small = np.array([0.3892,0.4236,0.3645,0.3695,0.4384,0.4039,0.3842,0.4039,0.399,0.3744])\n",
    "cora_syn = np.array([0.5699,0.5956,0.5882,0.5827,0.5809,0.5993,0.5827,0.5901,0.5864,0.6103])\n",
    "cora = np.array([0.622,0.612,0.612,0.615,0.639,0.642,0.593,0.638,0.601,0.614])\n",
    "rs = np.array([0.8010, 0.8010, 0.8180, 0.8070, 0.8060, 0.8080, 0.8020, 0.8080, 0.8120, 0.7980])\n",
    "\n",
    "\n",
    "large_mean = np.mean(large)\n",
    "medium_mean = np.mean(medium)\n",
    "small_mean = np.mean(small)\n",
    "syn_mean = np.mean(cora_syn)\n",
    "cora_mean = np.mean(cora)\n",
    "rs_mean = np.mean(rs)\n",
    "\n",
    "large_std = np.std(large)\n",
    "medium_std = np.std(medium)\n",
    "small_std = np.std(small)\n",
    "syn_std = np.std(cora_syn)\n",
    "cora_std = np.std(cora)\n",
    "rs_std = np.std(rs)\n",
    "\n",
    "graphs = ['KG Small', 'KG Medium', 'KG Small', 'Cora-syn', 'Cora']\n",
    "x_pos = np.arange(len(graphs))\n",
    "CTEs = [small_mean, medium_mean, large_mean, syn_mean, cora_mean]\n",
    "error = [small_std, medium_std, large_std, syn_std, cora_std]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.bar(x_pos, CTEs, yerr=error, align='center', alpha=0.5, ecolor='black', capsize=10)\n",
    "ax.set_ylabel('Average accuracy')\n",
    "ax.set_xlabel('Graph type')\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels(graphs)\n",
    "ax.set_title('Perfomance results')\n",
    "ax.yaxis.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
